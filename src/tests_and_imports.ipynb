{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: playable with RL\n",
    "    # CHECK: CNN structure + other info (check with stable baselines maybe)\n",
    "    # TODO: fix multidiscrete / multimodal input compatibility\n",
    "\n",
    "# TODO: test without prioritized experience replay first (epsilon greedy)\n",
    "# TODO: prioritized experience replay implem\n",
    "\n",
    "\n",
    "\n",
    "# TODO: save personal gameplay\n",
    "    # press anything to start\n",
    "    # reset if R pressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncated\n"
     ]
    }
   ],
   "source": [
    "from game_gym_env import CarGameEnv\n",
    "\n",
    "env = CarGameEnv(render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "action_space = env.action_space\n",
    "done = False\n",
    "while not done:\n",
    "    action = action_space.sample()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    done = terminated or truncated\n",
    "    if terminated:\n",
    "        print(\"terminated\")\n",
    "    if truncated:\n",
    "        print(\"truncated\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 15, 7])\n",
      "torch.Size([1, 32, 7, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 3, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_in = torch.zeros((1,1,30,15))\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "x = pool(F.relu(conv1(test_in))) # 16*30*15 -> 16*15*8\n",
    "print(x.size())\n",
    "x = pool(F.relu(conv2(x))) # 32*15*8 -> 32*8*4\n",
    "print(x.size())\n",
    "x = pool(F.relu(conv3(x))) # 64*8*4 -> 64*4*2\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # need to sample differently, see prioritized experience replay Schaul et al. 2016\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game_gym_env import CarGameEnv\n",
    "import pygame\n",
    "\n",
    "env = CarGameEnv()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc_size = 64*3*1\n",
    "        self.fc_image = nn.Linear(self.fc_size, 128)  # Flatten image output\n",
    "        self.fc_scalar = nn.Linear(2, 32)  # 2 scalar features\n",
    "        self.fc_merge = nn.Linear(128 + 32, 256)\n",
    "        self.fc_out = nn.Linear(256, sum([3,2,2,2]))  # Output Q-values for each discrete action\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, image, scalar):\n",
    "        # image: 1*30*15\n",
    "        x = self.pool(F.relu(self.conv1(image))) # 16*30*15 -> 16*15*7\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 32*15*7 -> 32*7*3\n",
    "        x = self.pool(F.relu(self.conv3(x))) # 64*7*3 -> 64*3*1\n",
    "        x = x.view(-1, self.fc_size)\n",
    "        x = F.relu(self.fc_image(x))\n",
    "\n",
    "        s = F.relu(self.fc_scalar(scalar))\n",
    "        print(x.shape, s.shape)\n",
    "        merged = torch.cat((x, s), dim=1)\n",
    "        merged = F.relu(self.fc_merge(merged))\n",
    "        q_values = self.fc_out(merged)\n",
    "        return torch.split(q_values, [3, 2, 2, 2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "REPLAY_BUFF_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128]) torch.Size([1, 32])\n",
      "action: [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# modified from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "# from utils import state_batch_to_tensor\n",
    "from config import *\n",
    "import numpy as np\n",
    "def state_batch_to_tensor(state_batch):\n",
    "    images = torch.tensor(np.array([s[\"image\"] / 255.0 for s in state_batch])).to(torch.float32)\n",
    "    scalars = torch.tensor(np.array([[s[\"speed\"].item() / CAR_TOP_SPEED, \n",
    "                                      s[\"boostsLeft\"] / CAR_INIT_SPEED_BOOST_COUNT] \n",
    "                                    for s in state_batch])).to(torch.float32)\n",
    "    return images, scalars\n",
    "\n",
    "# from rl_hyperparams import *\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(REPLAY_BUFF_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    # epsilon-greedy approach\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if True: #TESTING\n",
    "    # if sample > eps_threshold:\n",
    "        print(\"STILL TESTING, EPSILON GREEDY NOT YET\")\n",
    "        image, scalar = state_batch_to_tensor([state])\n",
    "        q_values = policy_net(image, scalar)\n",
    "        with torch.no_grad():\n",
    "            action = [torch.argmax(q, dim=1).detach().cpu().numpy()[0] for q in q_values]  # Take highest Q-value per action\n",
    "            return action\n",
    "            # return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "test_state = env.observation_space.sample()\n",
    "print(\"action:\", select_action(test_state))\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_img_batch, state_scr_batch = state_batch_to_tensor(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    print(\"CHECK STATE ACTION VALUES\")\n",
    "    state_action_values = policy_net(state_img_batch, state_scr_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values # how does this work with multidiscrete\n",
    "    # Compute the expected Q values\n",
    "    next_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, next_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # episode_durations.append(t + 1)\n",
    "            # plot_durations()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from car_physics import Car\n",
    "from checkpoint import generateCheckpoints\n",
    "from utils import get_rl_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def debugMask(mask):\n",
    "    resized = mask.scale((4, 4))\n",
    "    width, height = resized.get_size()\n",
    "    result = np.zeros((width, height))\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            result[x,y] = mask.get_at((x,y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from racing_game import Game\n",
    "from config import *\n",
    "game = Game()\n",
    "game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "isAccelerating = False\n",
    "isBreaking = False\n",
    "steerDirection = 1\n",
    "tryBoosting = False\n",
    "\n",
    "score = 0 # curr checkpoint\n",
    "timer = 0\n",
    "\n",
    "gameStarted = False # press to start\n",
    "\n",
    "maxSpeed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19988\n",
      "1.954787720496692\n",
      "1.954787720496692\n",
      "1.954787720496692\n",
      "1.954787720496692\n",
      "3.1623813668819154\n",
      "3.1623813668819154\n",
      "3.1623813668819154\n",
      "3.1623813668819154\n",
      "3.1623813668819154\n",
      "3.1623813668819154\n",
      "3.1623813668819154\n",
      "3.20760214625358\n",
      "3.4967443735969588\n",
      "3.4967443735969588\n",
      "3.4967443735969588\n",
      "3.4967443735969588\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "3.530878832402872\n",
      "6.1155894720685\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "6.418889331531042\n",
      "7.180450033090608\n",
      "7.180450033090608\n",
      "7.180450033090608\n",
      "7.180450033090608\n",
      "7.180450033090608\n",
      "7.180450033090608\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\cours\\ENSTA\\P2\\RL\\rl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if not gameStarted:\n",
    "        if pygame.event.peek(pygame.KEYDOWN):\n",
    "            gameStarted = True\n",
    "    if not gameStarted:\n",
    "        game.display()\n",
    "        continue\n",
    "    # Process player inputs.\n",
    "    tryBoosting = False\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            raise SystemExit\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            # Get mouse position\n",
    "            mouse_pos = pygame.mouse.get_pos()\n",
    "            print(f\"Mouse clicked at: {mouse_pos}\")\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_LEFT: \n",
    "                steerDirection += 1\n",
    "            if event.key == pygame.K_RIGHT:\n",
    "                steerDirection -= 1\n",
    "            if event.key == pygame.K_UP:\n",
    "                isAccelerating = True\n",
    "            if event.key == pygame.K_DOWN:\n",
    "                isBreaking = True\n",
    "            if event.key == pygame.K_SPACE:\n",
    "                tryBoosting = True\n",
    "            if event.key == pygame.K_PLUS:\n",
    "                game.car.BASE_ACCEL += 0.01\n",
    "                print(game.car.BASE_ACCEL)\n",
    "            if event.key == pygame.K_MINUS:\n",
    "                game.car.BASE_ACCEL -= 0.01\n",
    "                print(game.car.BASE_ACCEL)\n",
    "        if event.type == pygame.KEYUP:\n",
    "            if event.key == pygame.K_LEFT: \n",
    "                steerDirection -= 1\n",
    "            if event.key == pygame.K_RIGHT:\n",
    "                steerDirection += 1\n",
    "            if event.key == pygame.K_UP:\n",
    "                isAccelerating = False\n",
    "            if event.key == pygame.K_DOWN:\n",
    "                isBreaking = False\n",
    "    game.inputProcessing(steerDirection, isAccelerating, isBreaking, tryBoosting)\n",
    "    maxSpeed = max(maxSpeed, game.car.vel)\n",
    "    if game.timer % FPS_RATE == 0:\n",
    "        print(maxSpeed)\n",
    "    game.display(DEBUG=True)\n",
    "    game.timeUpdate(framerate=FPS_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
